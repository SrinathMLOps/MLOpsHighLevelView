{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”§ Phase 3: Feature Engineering\n",
        "\n",
        "This notebook demonstrates comprehensive feature engineering techniques for MLOps, covering feature extraction, selection, and transformation.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Feature Extraction](#1-feature-extraction)\n",
        "2. [Feature Selection](#2-feature-selection)\n",
        "3. [Feature Transformation](#3-feature-transformation)\n",
        "4. [Advanced Feature Engineering](#4-advanced-feature-engineering)\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "Make sure you have the required libraries installed:\n",
        "```bash\n",
        "pip install pandas numpy scikit-learn matplotlib seaborn plotly\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")\n",
        "\n",
        "# Load data (create sample if not available)\n",
        "try:\n",
        "    df = pd.read_parquet('data/processed/dataset_v*.parquet')\n",
        "    print(\"âœ… Loaded processed data\")\n",
        "except:\n",
        "    print(\"âš ï¸  Creating sample data...\")\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1000\n",
        "    \n",
        "    data = {\n",
        "        'customer_id': range(1, n_samples + 1),\n",
        "        'age': np.random.normal(35, 12, n_samples).astype(int),\n",
        "        'income': np.random.lognormal(10, 0.5, n_samples),\n",
        "        'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples, p=[0.3, 0.4, 0.2, 0.1]),\n",
        "        'employment_status': np.random.choice(['Employed', 'Unemployed', 'Self-employed', 'Retired'], n_samples, p=[0.6, 0.1, 0.2, 0.1]),\n",
        "        'credit_score': np.random.normal(650, 100, n_samples).astype(int),\n",
        "        'loan_amount': np.random.exponential(50000, n_samples),\n",
        "        'default_risk': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
        "        'application_date': pd.date_range('2020-01-01', periods=n_samples, freq='D'),\n",
        "        'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_samples, p=[0.2, 0.15, 0.15, 0.15, 0.35]),\n",
        "        'marital_status': np.random.choice(['Single', 'Married', 'Divorced', 'Widowed'], n_samples, p=[0.4, 0.4, 0.15, 0.05]),\n",
        "        'dependents': np.random.poisson(1.5, n_samples),\n",
        "        'previous_loans': np.random.poisson(2, n_samples),\n",
        "        'late_payments': np.random.poisson(0.5, n_samples),\n",
        "        'debt_to_income_ratio': np.random.beta(2, 5, n_samples),\n",
        "        'credit_utilization': np.random.beta(3, 2, n_samples),\n",
        "        'home_ownership': np.random.choice(['Rent', 'Own', 'Mortgage'], n_samples, p=[0.4, 0.2, 0.4]),\n",
        "        'purpose': np.random.choice(['Debt Consolidation', 'Home Improvement', 'Business', 'Education'], n_samples, p=[0.4, 0.2, 0.2, 0.2])\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"âœ… Sample dataset created\")\n",
        "\n",
        "print(f\"ðŸ“Š Dataset shape: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Feature Extraction\n",
        "\n",
        "**Purpose**: Extract meaningful features from raw data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1 Extract Date/Time Features\n",
        "print(\"ðŸ”§ Step 10: Feature Engineering\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Extract date/time features\n",
        "if 'application_date' in df.columns:\n",
        "    df['year'] = df['application_date'].dt.year\n",
        "    df['month'] = df['application_date'].dt.month\n",
        "    df['day_of_week'] = df['application_date'].dt.dayofweek\n",
        "    df['is_weekend'] = df['day_of_week'].isin([5, 6])\n",
        "    df['quarter'] = df['application_date'].dt.quarter\n",
        "    print(\"âœ… Extracted date/time features\")\n",
        "\n",
        "# 1.2 Extract Text Features\n",
        "if 'city' in df.columns:\n",
        "    df['city_length'] = df['city'].str.len()\n",
        "    df['is_major_city'] = df['city'].isin(['New York', 'Los Angeles', 'Chicago'])\n",
        "    print(\"âœ… Extracted text features\")\n",
        "\n",
        "# 1.3 Extract Numerical Features\n",
        "# Create age groups\n",
        "df['age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 45, 55, 65, 100], \n",
        "                        labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
        "\n",
        "# Create income brackets\n",
        "df['income_bracket'] = pd.cut(df['income'], \n",
        "                            bins=[0, 30000, 50000, 75000, 100000, float('inf')], \n",
        "                            labels=['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High'])\n",
        "\n",
        "# Create credit score categories\n",
        "df['credit_rating'] = pd.cut(df['credit_score'], \n",
        "                           bins=[0, 580, 670, 740, 800, 850], \n",
        "                           labels=['Poor', 'Fair', 'Good', 'Very Good', 'Excellent'])\n",
        "\n",
        "print(\"âœ… Extracted numerical features\")\n",
        "\n",
        "# 1.4 Create Derived Features\n",
        "# Loan to income ratio\n",
        "df['loan_to_income_ratio'] = df['loan_amount'] / df['income']\n",
        "\n",
        "# Credit utilization impact\n",
        "df['credit_utilization_impact'] = df['credit_utilization'] * df['credit_score']\n",
        "\n",
        "# Risk score\n",
        "df['risk_score'] = (df['late_payments'] * 10) + (df['debt_to_income_ratio'] * 100) - df['credit_score']\n",
        "\n",
        "print(\"âœ… Created derived features\")\n",
        "print(f\"New dataset shape: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Selection\n",
        "\n",
        "**Purpose**: Select the most relevant features for model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1 Statistical Feature Selection\n",
        "print(\"\\nðŸŽ¯ Feature Selection\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Prepare features and target\n",
        "X = df.drop(['customer_id', 'default_risk', 'application_date'], axis=1, errors='ignore')\n",
        "y = df['default_risk']\n",
        "\n",
        "# Handle categorical variables for feature selection\n",
        "X_encoded = X.copy()\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Label encode categorical variables\n",
        "le_dict = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
        "    le_dict[col] = le\n",
        "\n",
        "print(f\"Features before selection: {X_encoded.shape[1]}\")\n",
        "\n",
        "# 2.2 Univariate Feature Selection\n",
        "# Select top 10 features using f_classif\n",
        "selector_f = SelectKBest(score_func=f_classif, k=10)\n",
        "X_selected_f = selector_f.fit_transform(X_encoded, y)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features_f = X_encoded.columns[selector_f.get_support()].tolist()\n",
        "print(f\"Top 10 features (f_classif): {selected_features_f}\")\n",
        "\n",
        "# Mutual information feature selection\n",
        "selector_mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
        "X_selected_mi = selector_mi.fit_transform(X_encoded, y)\n",
        "\n",
        "selected_features_mi = X_encoded.columns[selector_mi.get_support()].tolist()\n",
        "print(f\"Top 10 features (mutual_info): {selected_features_mi}\")\n",
        "\n",
        "# 2.3 Tree-based Feature Selection\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_encoded, y)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_encoded.columns,\n",
        "    'importance': rf.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\nTop 10 features (Random Forest):\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# Select top features based on importance threshold\n",
        "importance_threshold = 0.01\n",
        "selected_features_rf = feature_importance[feature_importance['importance'] > importance_threshold]['feature'].tolist()\n",
        "print(f\"Features with importance > {importance_threshold}: {len(selected_features_rf)}\")\n",
        "\n",
        "# 2.4 Correlation-based Feature Selection\n",
        "correlation_matrix = X_encoded.corr()\n",
        "high_corr_pairs = []\n",
        "\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_val = abs(correlation_matrix.iloc[i, j])\n",
        "        if corr_val > 0.8:  # High correlation threshold\n",
        "            high_corr_pairs.append((\n",
        "                correlation_matrix.columns[i],\n",
        "                correlation_matrix.columns[j],\n",
        "                corr_val\n",
        "            ))\n",
        "\n",
        "print(f\"\\nHigh correlation pairs (>0.8): {len(high_corr_pairs)}\")\n",
        "for pair in high_corr_pairs[:5]:  # Show first 5\n",
        "    print(f\"{pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
        "\n",
        "# Remove highly correlated features\n",
        "features_to_remove = set()\n",
        "for pair in high_corr_pairs:\n",
        "    # Remove the feature with lower importance\n",
        "    if pair[0] in feature_importance['feature'].values and pair[1] in feature_importance['feature'].values:\n",
        "        imp1 = feature_importance[feature_importance['feature'] == pair[0]]['importance'].iloc[0]\n",
        "        imp2 = feature_importance[feature_importance['feature'] == pair[1]]['importance'].iloc[0]\n",
        "        if imp1 < imp2:\n",
        "            features_to_remove.add(pair[0])\n",
        "        else:\n",
        "            features_to_remove.add(pair[1])\n",
        "\n",
        "print(f\"Features to remove due to high correlation: {len(features_to_remove)}\")\n",
        "\n",
        "# Final feature selection\n",
        "final_features = [f for f in selected_features_rf if f not in features_to_remove]\n",
        "print(f\"Final selected features: {len(final_features)}\")\n",
        "print(f\"Features: {final_features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Transformation\n",
        "\n",
        "**Purpose**: Transform features to improve model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1 One-Hot Encoding for Categorical Variables\n",
        "print(\"\\nðŸ”„ Feature Transformation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create final dataset with selected features\n",
        "X_final = X[final_features].copy()\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "categorical_features = X_final.select_dtypes(include=['object', 'category']).columns\n",
        "numerical_features = X_final.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "print(f\"Categorical features to encode: {list(categorical_features)}\")\n",
        "print(f\"Numerical features: {list(numerical_features)}\")\n",
        "\n",
        "# Apply one-hot encoding\n",
        "X_encoded_final = pd.get_dummies(X_final, columns=categorical_features, prefix=categorical_features)\n",
        "print(f\"Features after one-hot encoding: {X_encoded_final.shape[1]}\")\n",
        "\n",
        "# 3.2 Scaling Numerical Features\n",
        "print(\"\\nðŸ“ Scaling Numerical Features\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Standard Scaling\n",
        "scaler_standard = StandardScaler()\n",
        "X_standard = X_encoded_final.copy()\n",
        "X_standard[numerical_features] = scaler_standard.fit_transform(X_standard[numerical_features])\n",
        "\n",
        "print(\"âœ… Applied Standard Scaling\")\n",
        "\n",
        "# Min-Max Scaling\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_minmax = X_encoded_final.copy()\n",
        "X_minmax[numerical_features] = scaler_minmax.fit_transform(X_minmax[numerical_features])\n",
        "\n",
        "print(\"âœ… Applied Min-Max Scaling\")\n",
        "\n",
        "# 3.3 Polynomial Features\n",
        "print(\"\\nðŸ”¢ Polynomial Features\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Create polynomial features for numerical columns\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
        "X_poly = poly.fit_transform(X_encoded_final[numerical_features])\n",
        "X_poly_df = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(numerical_features))\n",
        "\n",
        "print(f\"Polynomial features created: {X_poly_df.shape[1]}\")\n",
        "print(f\"Original numerical features: {len(numerical_features)}\")\n",
        "\n",
        "# 3.4 Feature Engineering Summary\n",
        "print(\"\\nðŸ“Š Feature Engineering Summary\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "print(f\"Features after extraction: {X.shape[1]}\")\n",
        "print(f\"Features after selection: {X_final.shape[1]}\")\n",
        "print(f\"Features after encoding: {X_encoded_final.shape[1]}\")\n",
        "print(f\"Features after polynomial: {X_poly_df.shape[1]}\")\n",
        "\n",
        "# Save processed features\n",
        "feature_engineering_results = {\n",
        "    'X_original': X,\n",
        "    'X_selected': X_final,\n",
        "    'X_encoded': X_encoded_final,\n",
        "    'X_standard': X_standard,\n",
        "    'X_minmax': X_minmax,\n",
        "    'X_polynomial': X_poly_df,\n",
        "    'selected_features': final_features,\n",
        "    'categorical_features': list(categorical_features),\n",
        "    'numerical_features': list(numerical_features),\n",
        "    'scalers': {\n",
        "        'standard': scaler_standard,\n",
        "        'minmax': scaler_minmax\n",
        "    },\n",
        "    'label_encoders': le_dict\n",
        "}\n",
        "\n",
        "print(\"âœ… Feature engineering completed successfully!\")\n",
        "print(\"âœ… All transformations saved for model training\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
