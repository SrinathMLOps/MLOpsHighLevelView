{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Phase 6: Model Monitoring & Improvement\n",
        "\n",
        "This notebook demonstrates comprehensive model monitoring and improvement techniques for MLOps, covering performance tracking, drift detection, and model lifecycle management.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Model Monitoring](#1-model-monitoring)\n",
        "2. [Drift Detection](#2-drift-detection)\n",
        "3. [Performance Tracking](#3-performance-tracking)\n",
        "4. [Model Lifecycle Management](#4-model-lifecycle-management)\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "Make sure you have the required libraries installed:\n",
        "```bash\n",
        "pip install pandas numpy matplotlib seaborn plotly scipy scikit-learn\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from scipy import stats\n",
        "import warnings\n",
        "import json\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "\n",
        "# Load sample data for monitoring\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Simulate production data\n",
        "production_data = {\n",
        "    'timestamp': pd.date_range('2024-01-01', periods=n_samples, freq='H'),\n",
        "    'age': np.random.normal(35, 12, n_samples).astype(int),\n",
        "    'income': np.random.lognormal(10, 0.5, n_samples),\n",
        "    'credit_score': np.random.normal(650, 100, n_samples).astype(int),\n",
        "    'loan_amount': np.random.exponential(50000, n_samples),\n",
        "    'prediction': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
        "    'actual': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
        "    'probability': np.random.beta(2, 5, n_samples)\n",
        "}\n",
        "\n",
        "df_production = pd.DataFrame(production_data)\n",
        "print(f\"üìä Production data shape: {df_production.shape}\")\n",
        "print(f\"Date range: {df_production['timestamp'].min()} to {df_production['timestamp'].max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Model Monitoring\n",
        "\n",
        "**Purpose**: Monitor model performance and detect issues in production.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1 Step 22: Monitor Model (Drift, Latency, Accuracy)\n",
        "print(\"üìä Step 22: Monitor Model\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Performance monitoring\n",
        "def calculate_performance_metrics(y_true, y_pred, y_proba):\n",
        "    \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
        "        'prediction_count': len(y_pred),\n",
        "        'positive_rate': np.mean(y_pred),\n",
        "        'avg_probability': np.mean(y_proba)\n",
        "    }\n",
        "\n",
        "# Calculate current performance\n",
        "current_metrics = calculate_performance_metrics(\n",
        "    df_production['actual'], \n",
        "    df_production['prediction'], \n",
        "    df_production['probability']\n",
        ")\n",
        "\n",
        "print(\"Current Performance Metrics:\")\n",
        "for metric, value in current_metrics.items():\n",
        "    print(f\"  {metric}: {value:.3f}\")\n",
        "\n",
        "# 1.2 Data Drift Detection\n",
        "print(\"\\nüîç Data Drift Detection\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Simulate baseline data (training data)\n",
        "baseline_data = {\n",
        "    'age': np.random.normal(35, 12, 1000),\n",
        "    'income': np.random.lognormal(10, 0.5, 1000),\n",
        "    'credit_score': np.random.normal(650, 100, 1000),\n",
        "    'loan_amount': np.random.exponential(50000, 1000)\n",
        "}\n",
        "\n",
        "df_baseline = pd.DataFrame(baseline_data)\n",
        "\n",
        "def detect_drift_ks(baseline, production, feature_name, alpha=0.05):\n",
        "    \"\"\"Detect drift using Kolmogorov-Smirnov test\"\"\"\n",
        "    statistic, p_value = stats.ks_2samp(baseline, production)\n",
        "    is_drift = p_value < alpha\n",
        "    return {\n",
        "        'feature': feature_name,\n",
        "        'ks_statistic': statistic,\n",
        "        'p_value': p_value,\n",
        "        'is_drift': is_drift,\n",
        "        'drift_severity': 'high' if p_value < 0.01 else 'medium' if p_value < 0.05 else 'low'\n",
        "    }\n",
        "\n",
        "# Check drift for each feature\n",
        "drift_results = []\n",
        "for feature in ['age', 'income', 'credit_score', 'loan_amount']:\n",
        "    drift_result = detect_drift_ks(\n",
        "        df_baseline[feature], \n",
        "        df_production[feature], \n",
        "        feature\n",
        "    )\n",
        "    drift_results.append(drift_result)\n",
        "\n",
        "# Display drift results\n",
        "drift_df = pd.DataFrame(drift_results)\n",
        "print(\"Data Drift Analysis:\")\n",
        "print(drift_df[['feature', 'is_drift', 'drift_severity', 'p_value']].round(4))\n",
        "\n",
        "# 1.3 Performance Drift Detection\n",
        "print(\"\\nüìà Performance Drift Detection\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Calculate performance over time windows\n",
        "df_production['hour'] = df_production['timestamp'].dt.hour\n",
        "df_production['day'] = df_production['timestamp'].dt.date\n",
        "\n",
        "# Performance by hour\n",
        "hourly_performance = df_production.groupby('hour').apply(\n",
        "    lambda x: calculate_performance_metrics(x['actual'], x['prediction'], x['probability'])\n",
        ").apply(pd.Series)\n",
        "\n",
        "print(\"Performance by Hour (sample):\")\n",
        "print(hourly_performance[['accuracy', 'f1_score']].head())\n",
        "\n",
        "# 1.4 Latency Monitoring\n",
        "print(\"\\n‚è±Ô∏è Latency Monitoring\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Simulate prediction latencies\n",
        "np.random.seed(42)\n",
        "latencies = np.random.exponential(50, len(df_production))  # milliseconds\n",
        "df_production['latency_ms'] = latencies\n",
        "\n",
        "latency_stats = {\n",
        "    'mean_latency': np.mean(latencies),\n",
        "    'median_latency': np.median(latencies),\n",
        "    'p95_latency': np.percentile(latencies, 95),\n",
        "    'p99_latency': np.percentile(latencies, 99),\n",
        "    'max_latency': np.max(latencies)\n",
        "}\n",
        "\n",
        "print(\"Latency Statistics (ms):\")\n",
        "for stat, value in latency_stats.items():\n",
        "    print(f\"  {stat}: {value:.2f}\")\n",
        "\n",
        "# Alert thresholds\n",
        "latency_thresholds = {\n",
        "    'warning': 100,  # ms\n",
        "    'critical': 500  # ms\n",
        "}\n",
        "\n",
        "high_latency_count = np.sum(latencies > latency_thresholds['warning'])\n",
        "print(f\"\\nHigh latency predictions (>100ms): {high_latency_count}\")\n",
        "\n",
        "print(\"‚úÖ Model monitoring completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Lifecycle Management\n",
        "\n",
        "**Purpose**: Manage model retraining and retirement decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1 Step 23: Retrain or Retire Model\n",
        "print(\"üîÑ Step 23: Retrain or Retire Model\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Define retraining triggers\n",
        "def evaluate_retraining_triggers(performance_metrics, drift_results, latency_stats):\n",
        "    \"\"\"Evaluate if model needs retraining based on various triggers\"\"\"\n",
        "    triggers = {\n",
        "        'performance_degradation': False,\n",
        "        'data_drift': False,\n",
        "        'high_latency': False,\n",
        "        'time_based': False\n",
        "    }\n",
        "    \n",
        "    # Performance degradation trigger\n",
        "    if performance_metrics['accuracy'] < 0.7:  # Threshold\n",
        "        triggers['performance_degradation'] = True\n",
        "        print(\"‚ö†Ô∏è  Performance degradation detected!\")\n",
        "    \n",
        "    # Data drift trigger\n",
        "    high_drift_features = [r for r in drift_results if r['drift_severity'] == 'high']\n",
        "    if len(high_drift_features) > 0:\n",
        "        triggers['data_drift'] = True\n",
        "        print(f\"‚ö†Ô∏è  Data drift detected in {len(high_drift_features)} features!\")\n",
        "    \n",
        "    # Latency trigger\n",
        "    if latency_stats['p95_latency'] > 200:  # 200ms threshold\n",
        "        triggers['high_latency'] = True\n",
        "        print(\"‚ö†Ô∏è  High latency detected!\")\n",
        "    \n",
        "    # Time-based trigger (simulate)\n",
        "    days_since_training = 30  # Simulated\n",
        "    if days_since_training > 90:  # 90 days threshold\n",
        "        triggers['time_based'] = True\n",
        "        print(\"‚ö†Ô∏è  Model is older than 90 days!\")\n",
        "    \n",
        "    return triggers\n",
        "\n",
        "# Evaluate retraining triggers\n",
        "retraining_triggers = evaluate_retraining_triggers(\n",
        "    current_metrics, \n",
        "    drift_results, \n",
        "    latency_stats\n",
        ")\n",
        "\n",
        "print(\"\\nRetraining Trigger Analysis:\")\n",
        "for trigger, status in retraining_triggers.items():\n",
        "    status_icon = \"üî¥\" if status else \"üü¢\"\n",
        "    print(f\"  {status_icon} {trigger}: {status}\")\n",
        "\n",
        "# 2.2 Retraining Decision Logic\n",
        "print(\"\\nü§î Retraining Decision\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "def make_retraining_decision(triggers):\n",
        "    \"\"\"Make decision on whether to retrain or retire model\"\"\"\n",
        "    critical_triggers = sum([\n",
        "        triggers['performance_degradation'],\n",
        "        len([r for r in drift_results if r['drift_severity'] == 'high']) > 2\n",
        "    ])\n",
        "    \n",
        "    warning_triggers = sum([\n",
        "        triggers['data_drift'],\n",
        "        triggers['high_latency'],\n",
        "        triggers['time_based']\n",
        "    ])\n",
        "    \n",
        "    if critical_triggers >= 1:\n",
        "        decision = \"RETRAIN_IMMEDIATELY\"\n",
        "        action = \"Schedule immediate retraining with fresh data\"\n",
        "    elif warning_triggers >= 2:\n",
        "        decision = \"RETRAIN_SOON\"\n",
        "        action = \"Schedule retraining within 1 week\"\n",
        "    elif warning_triggers >= 1:\n",
        "        decision = \"MONITOR_CLOSELY\"\n",
        "        action = \"Increase monitoring frequency\"\n",
        "    else:\n",
        "        decision = \"CONTINUE\"\n",
        "        action = \"Model performing well, continue monitoring\"\n",
        "    \n",
        "    return decision, action\n",
        "\n",
        "decision, action = make_retraining_decision(retraining_triggers)\n",
        "\n",
        "print(f\"Decision: {decision}\")\n",
        "print(f\"Action: {action}\")\n",
        "\n",
        "# 2.3 Model Retirement Criteria\n",
        "print(\"\\nüíÄ Model Retirement Criteria\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "def evaluate_retirement_criteria(performance_metrics, triggers):\n",
        "    \"\"\"Evaluate if model should be retired\"\"\"\n",
        "    retirement_criteria = {\n",
        "        'performance_below_threshold': performance_metrics['accuracy'] < 0.6,\n",
        "        'multiple_critical_issues': sum([\n",
        "            triggers['performance_degradation'],\n",
        "            len([r for r in drift_results if r['drift_severity'] == 'high']) > 3\n",
        "        ]) >= 2,\n",
        "        'model_age': 365,  # Simulated model age in days\n",
        "        'replacement_available': True  # Simulated\n",
        "    }\n",
        "    \n",
        "    should_retire = (\n",
        "        retirement_criteria['performance_below_threshold'] or\n",
        "        retirement_criteria['multiple_critical_issues'] or\n",
        "        (retirement_criteria['model_age'] > 365 and retirement_criteria['replacement_available'])\n",
        "    )\n",
        "    \n",
        "    return should_retire, retirement_criteria\n",
        "\n",
        "should_retire, retirement_criteria = evaluate_retirement_criteria(current_metrics, retraining_triggers)\n",
        "\n",
        "print(\"Retirement Criteria Evaluation:\")\n",
        "for criterion, status in retirement_criteria.items():\n",
        "    status_icon = \"üî¥\" if status else \"üü¢\"\n",
        "    print(f\"  {status_icon} {criterion}: {status}\")\n",
        "\n",
        "if should_retire:\n",
        "    print(\"\\nüö® RECOMMENDATION: RETIRE MODEL\")\n",
        "    print(\"Model should be retired and replaced with a new version\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ RECOMMENDATION: CONTINUE WITH CURRENT MODEL\")\n",
        "    print(\"Model is performing adequately\")\n",
        "\n",
        "# 2.4 Monitoring Dashboard Summary\n",
        "print(\"\\nüìä Monitoring Dashboard Summary\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "dashboard_summary = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'model_status': 'active',\n",
        "    'performance_metrics': current_metrics,\n",
        "    'drift_detected': any([r['is_drift'] for r in drift_results]),\n",
        "    'latency_status': 'normal' if latency_stats['p95_latency'] < 200 else 'high',\n",
        "    'retraining_decision': decision,\n",
        "    'retirement_recommendation': should_retire,\n",
        "    'alerts': []\n",
        "}\n",
        "\n",
        "# Add alerts\n",
        "if retraining_triggers['performance_degradation']:\n",
        "    dashboard_summary['alerts'].append(\"Performance degradation detected\")\n",
        "if retraining_triggers['data_drift']:\n",
        "    dashboard_summary['alerts'].append(\"Data drift detected\")\n",
        "if retraining_triggers['high_latency']:\n",
        "    dashboard_summary['alerts'].append(\"High latency detected\")\n",
        "\n",
        "print(\"Dashboard Summary:\")\n",
        "for key, value in dashboard_summary.items():\n",
        "    if key != 'performance_metrics':\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "# Save monitoring results\n",
        "os.makedirs('monitoring', exist_ok=True)\n",
        "with open('monitoring/dashboard_summary.json', 'w') as f:\n",
        "    json.dump(dashboard_summary, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\n‚úÖ Monitoring results saved to: monitoring/dashboard_summary.json\")\n",
        "print(\"‚úÖ Model lifecycle management completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
