{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Phase 1: Data Preparation with Pandas\n",
        "\n",
        "This notebook demonstrates the complete data preparation pipeline for MLOps, covering all 6 steps from data ingestion to curation.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Data Ingestion](#1-data-ingestion)\n",
        "2. [Data Validation](#2-data-validation)\n",
        "3. [Data Cleaning](#3-data-cleaning)\n",
        "4. [Data Standardization](#4-data-standardization)\n",
        "5. [Data Transformation](#5-data-transformation)\n",
        "6. [Data Curation](#6-data-curation)\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "Make sure you have the required libraries installed:\n",
        "```bash\n",
        "pip install pandas numpy scikit-learn matplotlib seaborn plotly requests beautifulsoup4 sqlalchemy\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "import sqlalchemy\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(f\"üìÖ Current date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Ingestion\n",
        "\n",
        "**Purpose**: Collect and consolidate data from various sources into a unified format.\n",
        "\n",
        "### 1.1 Create Sample Dataset\n",
        "Let's start by creating a comprehensive sample dataset that simulates real-world data from multiple sources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample dataset simulating customer data\n",
        "def create_sample_dataset(n_samples=1000):\n",
        "    \"\"\"Create a comprehensive sample dataset with various data types and issues\"\"\"\n",
        "    \n",
        "    # Set random seed for reproducibility\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Generate base data\n",
        "    data = {\n",
        "        'customer_id': range(1, n_samples + 1),\n",
        "        'age': np.random.normal(35, 12, n_samples).astype(int),\n",
        "        'income': np.random.lognormal(10, 0.5, n_samples),\n",
        "        'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD', 'Associate'], \n",
        "                                    n_samples, p=[0.3, 0.4, 0.2, 0.05, 0.05]),\n",
        "        'employment_status': np.random.choice(['Employed', 'Unemployed', 'Self-employed', 'Retired'], \n",
        "                                            n_samples, p=[0.6, 0.1, 0.2, 0.1]),\n",
        "        'credit_score': np.random.normal(650, 100, n_samples).astype(int),\n",
        "        'loan_amount': np.random.exponential(50000, n_samples),\n",
        "        'loan_duration': np.random.choice([12, 24, 36, 48, 60], n_samples, p=[0.2, 0.3, 0.3, 0.15, 0.05]),\n",
        "        'interest_rate': np.random.normal(8.5, 2, n_samples),\n",
        "        'default_risk': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
        "        'application_date': pd.date_range('2020-01-01', periods=n_samples, freq='D'),\n",
        "        'email': [f'customer{i}@{np.random.choice([\"gmail.com\", \"yahoo.com\", \"hotmail.com\", \"company.com\"])}' \n",
        "                 for i in range(1, n_samples + 1)],\n",
        "        'phone': [f'+1-{np.random.randint(100, 999)}-{np.random.randint(100, 999)}-{np.random.randint(1000, 9999)}' \n",
        "                 for _ in range(n_samples)],\n",
        "        'address': [f'{np.random.randint(100, 9999)} {np.random.choice([\"Main St\", \"Oak Ave\", \"Pine Rd\", \"Elm St\"])}' \n",
        "                   for _ in range(n_samples)],\n",
        "        'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia'], \n",
        "                               n_samples, p=[0.2, 0.15, 0.15, 0.15, 0.15, 0.2]),\n",
        "        'state': np.random.choice(['NY', 'CA', 'IL', 'TX', 'AZ', 'PA'], n_samples, p=[0.2, 0.15, 0.15, 0.15, 0.15, 0.2]),\n",
        "        'zip_code': [f'{np.random.randint(10000, 99999)}' for _ in range(n_samples)],\n",
        "        'marital_status': np.random.choice(['Single', 'Married', 'Divorced', 'Widowed'], \n",
        "                                         n_samples, p=[0.4, 0.4, 0.15, 0.05]),\n",
        "        'dependents': np.random.poisson(1.5, n_samples),\n",
        "        'previous_loans': np.random.poisson(2, n_samples),\n",
        "        'late_payments': np.random.poisson(0.5, n_samples),\n",
        "        'account_age_days': np.random.exponential(1000, n_samples).astype(int),\n",
        "        'monthly_income': np.random.lognormal(8.5, 0.4, n_samples),\n",
        "        'debt_to_income_ratio': np.random.beta(2, 5, n_samples),\n",
        "        'credit_utilization': np.random.beta(3, 2, n_samples),\n",
        "        'inquiries_last_6m': np.random.poisson(2, n_samples),\n",
        "        'public_records': np.random.choice([0, 1, 2, 3], n_samples, p=[0.7, 0.2, 0.08, 0.02]),\n",
        "        'employment_length_years': np.random.exponential(5, n_samples),\n",
        "        'home_ownership': np.random.choice(['Rent', 'Own', 'Mortgage', 'Other'], \n",
        "                                         n_samples, p=[0.4, 0.2, 0.35, 0.05]),\n",
        "        'purpose': np.random.choice(['Debt Consolidation', 'Home Improvement', 'Business', 'Education', 'Personal'], \n",
        "                                  n_samples, p=[0.4, 0.2, 0.15, 0.15, 0.1]),\n",
        "        'loan_status': np.random.choice(['Approved', 'Rejected', 'Pending'], \n",
        "                                      n_samples, p=[0.7, 0.2, 0.1])\n",
        "    }\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    # Introduce some data quality issues\n",
        "    # Missing values\n",
        "    missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
        "    df.loc[missing_indices, 'credit_score'] = np.nan\n",
        "    \n",
        "    missing_indices = np.random.choice(df.index, size=int(0.03 * len(df)), replace=False)\n",
        "    df.loc[missing_indices, 'employment_length_years'] = np.nan\n",
        "    \n",
        "    # Duplicates\n",
        "    duplicate_indices = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)\n",
        "    df = pd.concat([df, df.loc[duplicate_indices]], ignore_index=True)\n",
        "    \n",
        "    # Outliers\n",
        "    outlier_indices = np.random.choice(df.index, size=int(0.01 * len(df)), replace=False)\n",
        "    df.loc[outlier_indices, 'income'] *= 10  # Extreme income values\n",
        "    \n",
        "    # Inconsistent data\n",
        "    inconsistent_indices = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)\n",
        "    df.loc[inconsistent_indices, 'age'] = df.loc[inconsistent_indices, 'age'] + 100  # Impossible ages\n",
        "    \n",
        "    # Mixed data types\n",
        "    mixed_indices = np.random.choice(df.index, size=int(0.01 * len(df)), replace=False)\n",
        "    df.loc[mixed_indices, 'credit_score'] = 'N/A'  # String in numeric column\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Create the sample dataset\n",
        "print(\"üîÑ Creating sample dataset...\")\n",
        "df = create_sample_dataset(1000)\n",
        "print(f\"‚úÖ Dataset created successfully!\")\n",
        "print(f\"üìä Dataset shape: {df.shape}\")\n",
        "print(f\"üìã Columns: {list(df.columns)}\")\n",
        "\n",
        "# Display basic info\n",
        "print(\"\\nüìà Dataset Overview:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Demonstrate Different Data Ingestion Methods\n",
        "\n",
        "Let's show how to ingest data from various sources:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.2.1 CSV File Ingestion\n",
        "print(\"üìÅ CSV File Ingestion:\")\n",
        "print(\"```python\")\n",
        "print(\"df_csv = pd.read_csv('data.csv', encoding='utf-8', low_memory=False)\")\n",
        "print(\"```\")\n",
        "\n",
        "# Save sample data to CSV for demonstration\n",
        "df.to_csv('sample_data.csv', index=False)\n",
        "df_csv_demo = pd.read_csv('sample_data.csv')\n",
        "print(f\"‚úÖ CSV loaded: {df_csv_demo.shape}\")\n",
        "\n",
        "# 1.2.2 Excel File Ingestion\n",
        "print(\"\\nüìä Excel File Ingestion:\")\n",
        "print(\"```python\")\n",
        "print(\"df_excel = pd.read_excel('data.xlsx', sheet_name='Sheet1', engine='openpyxl')\")\n",
        "print(\"```\")\n",
        "\n",
        "# Save sample data to Excel for demonstration\n",
        "df.to_excel('sample_data.xlsx', index=False)\n",
        "df_excel_demo = pd.read_excel('sample_data.xlsx')\n",
        "print(f\"‚úÖ Excel loaded: {df_excel_demo.shape}\")\n",
        "\n",
        "# 1.2.3 JSON API Ingestion (Simulated)\n",
        "print(\"\\nüåê JSON API Ingestion:\")\n",
        "print(\"```python\")\n",
        "print(\"response = requests.get('https://api.example.com/data')\")\n",
        "print(\"df_json = pd.json_normalize(response.json())\")\n",
        "print(\"```\")\n",
        "\n",
        "# Simulate JSON data\n",
        "json_data = df.head(5).to_dict('records')\n",
        "print(f\"‚úÖ JSON data simulated: {len(json_data)} records\")\n",
        "\n",
        "# 1.2.4 Database Ingestion (Simulated)\n",
        "print(\"\\nüóÑÔ∏è Database Ingestion:\")\n",
        "print(\"```python\")\n",
        "print(\"engine = sqlalchemy.create_engine('postgresql://user:pass@localhost/db')\")\n",
        "print(\"df_sql = pd.read_sql('SELECT * FROM table', engine)\")\n",
        "print(\"```\")\n",
        "\n",
        "print(\"‚úÖ Database connection simulated\")\n",
        "\n",
        "# 1.2.5 Data Ingestion Best Practices\n",
        "print(\"\\nüí° Data Ingestion Best Practices:\")\n",
        "print(\"1. Always validate data during ingestion\")\n",
        "print(\"2. Use connection pooling for database connections\")\n",
        "print(\"3. Handle API rate limits and retries\")\n",
        "print(\"4. Implement data lineage tracking\")\n",
        "print(\"5. Use appropriate encoding (UTF-8)\")\n",
        "print(\"6. Handle large files with chunking\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Validation\n",
        "\n",
        "**Purpose**: Ensure data quality, consistency, and integrity before processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1 Comprehensive Data Validation Function\n",
        "def validate_data(df, expected_columns=None, type_validation=None):\n",
        "    \"\"\"\n",
        "    Comprehensive data validation function\n",
        "    \"\"\"\n",
        "    validation_results = {\n",
        "        'schema_compliance': True,\n",
        "        'data_types': True,\n",
        "        'missing_values': {},\n",
        "        'data_ranges': True,\n",
        "        'business_rules': True,\n",
        "        'issues': []\n",
        "    }\n",
        "    \n",
        "    print(\"üîç Starting Data Validation...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # 1. Schema Compliance Check\n",
        "    if expected_columns:\n",
        "        missing_cols = set(expected_columns) - set(df.columns)\n",
        "        if missing_cols:\n",
        "            validation_results['schema_compliance'] = False\n",
        "            validation_results['issues'].append(f\"Missing columns: {missing_cols}\")\n",
        "            print(f\"‚ùå Missing columns: {missing_cols}\")\n",
        "        else:\n",
        "            print(\"‚úÖ Schema compliance: All expected columns present\")\n",
        "    \n",
        "    # 2. Data Types Validation\n",
        "    if type_validation:\n",
        "        for col, expected_type in type_validation.items():\n",
        "            if col in df.columns:\n",
        "                actual_type = str(df[col].dtype)\n",
        "                if expected_type not in actual_type:\n",
        "                    validation_results['data_types'] = False\n",
        "                    validation_results['issues'].append(f\"{col}: expected {expected_type}, got {actual_type}\")\n",
        "                    print(f\"‚ùå {col}: expected {expected_type}, got {actual_type}\")\n",
        "                else:\n",
        "                    print(f\"‚úÖ {col}: correct data type ({actual_type})\")\n",
        "    \n",
        "    # 3. Missing Values Analysis\n",
        "    print(\"\\nüìä Missing Values Analysis:\")\n",
        "    missing_data = df.isnull().sum()\n",
        "    missing_percent = (missing_data / len(df)) * 100\n",
        "    \n",
        "    for col, count in missing_data.items():\n",
        "        if count > 0:\n",
        "            validation_results['missing_values'][col] = {\n",
        "                'count': count,\n",
        "                'percentage': missing_percent[col]\n",
        "            }\n",
        "            print(f\"‚ö†Ô∏è  {col}: {count} missing ({missing_percent[col]:.2f}%)\")\n",
        "        else:\n",
        "            print(f\"‚úÖ {col}: No missing values\")\n",
        "    \n",
        "    # 4. Data Range Validation\n",
        "    print(\"\\nüìè Data Range Validation:\")\n",
        "    \n",
        "    # Age validation\n",
        "    if 'age' in df.columns:\n",
        "        invalid_ages = df[(df['age'] < 0) | (df['age'] > 120)]\n",
        "        if len(invalid_ages) > 0:\n",
        "            validation_results['data_ranges'] = False\n",
        "            validation_results['issues'].append(f\"Invalid ages: {len(invalid_ages)} records\")\n",
        "            print(f\"‚ùå Invalid ages: {len(invalid_ages)} records\")\n",
        "        else:\n",
        "            print(\"‚úÖ Age values within valid range (0-120)\")\n",
        "    \n",
        "    # Income validation\n",
        "    if 'income' in df.columns:\n",
        "        negative_income = df[df['income'] < 0]\n",
        "        if len(negative_income) > 0:\n",
        "            validation_results['data_ranges'] = False\n",
        "            validation_results['issues'].append(f\"Negative income: {len(negative_income)} records\")\n",
        "            print(f\"‚ùå Negative income: {len(negative_income)} records\")\n",
        "        else:\n",
        "            print(\"‚úÖ Income values are non-negative\")\n",
        "    \n",
        "    # Credit score validation\n",
        "    if 'credit_score' in df.columns:\n",
        "        # Handle mixed data types\n",
        "        numeric_credit_scores = pd.to_numeric(df['credit_score'], errors='coerce')\n",
        "        invalid_scores = df[(numeric_credit_scores < 300) | (numeric_credit_scores > 850)]\n",
        "        if len(invalid_scores) > 0:\n",
        "            validation_results['data_ranges'] = False\n",
        "            validation_results['issues'].append(f\"Invalid credit scores: {len(invalid_scores)} records\")\n",
        "            print(f\"‚ùå Invalid credit scores: {len(invalid_scores)} records\")\n",
        "        else:\n",
        "            print(\"‚úÖ Credit scores within valid range (300-850)\")\n",
        "    \n",
        "    # 5. Business Rules Validation\n",
        "    print(\"\\nüíº Business Rules Validation:\")\n",
        "    \n",
        "    # Debt-to-income ratio validation\n",
        "    if 'debt_to_income_ratio' in df.columns:\n",
        "        high_dti = df[df['debt_to_income_ratio'] > 1.0]\n",
        "        if len(high_dti) > 0:\n",
        "            validation_results['business_rules'] = False\n",
        "            validation_results['issues'].append(f\"High debt-to-income ratio: {len(high_dti)} records\")\n",
        "            print(f\"‚ö†Ô∏è  High debt-to-income ratio: {len(high_dti)} records\")\n",
        "        else:\n",
        "            print(\"‚úÖ Debt-to-income ratios are reasonable\")\n",
        "    \n",
        "    # 6. Summary\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"üìã VALIDATION SUMMARY:\")\n",
        "    print(f\"Schema Compliance: {'‚úÖ' if validation_results['schema_compliance'] else '‚ùå'}\")\n",
        "    print(f\"Data Types: {'‚úÖ' if validation_results['data_types'] else '‚ùå'}\")\n",
        "    print(f\"Data Ranges: {'‚úÖ' if validation_results['data_ranges'] else '‚ùå'}\")\n",
        "    print(f\"Business Rules: {'‚úÖ' if validation_results['business_rules'] else '‚ùå'}\")\n",
        "    \n",
        "    if validation_results['issues']:\n",
        "        print(f\"\\n‚ö†Ô∏è  Issues Found: {len(validation_results['issues'])}\")\n",
        "        for i, issue in enumerate(validation_results['issues'], 1):\n",
        "            print(f\"  {i}. {issue}\")\n",
        "    else:\n",
        "        print(\"\\nüéâ No validation issues found!\")\n",
        "    \n",
        "    return validation_results\n",
        "\n",
        "# Define expected schema and data types\n",
        "expected_columns = ['customer_id', 'age', 'income', 'education', 'employment_status', \n",
        "                   'credit_score', 'loan_amount', 'default_risk']\n",
        "\n",
        "type_validation = {\n",
        "    'customer_id': 'int',\n",
        "    'age': 'int',\n",
        "    'income': 'float',\n",
        "    'credit_score': 'int',\n",
        "    'loan_amount': 'float',\n",
        "    'default_risk': 'int'\n",
        "}\n",
        "\n",
        "# Run validation\n",
        "validation_results = validate_data(df, expected_columns, type_validation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Cleaning\n",
        "\n",
        "**Purpose**: Handle missing values, duplicates, and data type conversions to ensure data quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1 Data Cleaning Pipeline\n",
        "def clean_data(df):\n",
        "    \"\"\"\n",
        "    Comprehensive data cleaning pipeline\n",
        "    \"\"\"\n",
        "    print(\"üßπ Starting Data Cleaning Process...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Create a copy to avoid modifying original data\n",
        "    df_clean = df.copy()\n",
        "    original_shape = df_clean.shape\n",
        "    \n",
        "    print(f\"üìä Original dataset shape: {original_shape}\")\n",
        "    \n",
        "    # 3.1.1 Handle Missing Values\n",
        "    print(\"\\nüîç Step 1: Handling Missing Values\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Check missing values before cleaning\n",
        "    missing_before = df_clean.isnull().sum()\n",
        "    print(\"Missing values before cleaning:\")\n",
        "    for col, count in missing_before.items():\n",
        "        if count > 0:\n",
        "            print(f\"  {col}: {count} ({count/len(df_clean)*100:.2f}%)\")\n",
        "    \n",
        "    # Strategy 1: Drop rows with missing critical data\n",
        "    critical_columns = ['customer_id', 'age', 'income', 'default_risk']\n",
        "    df_clean = df_clean.dropna(subset=critical_columns)\n",
        "    print(f\"\\n‚úÖ Dropped rows with missing critical data: {original_shape[0] - df_clean.shape[0]} rows\")\n",
        "    \n",
        "    # Strategy 2: Fill missing values with appropriate methods\n",
        "    # Numeric columns - use median\n",
        "    numeric_columns = df_clean.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_columns:\n",
        "        if df_clean[col].isnull().sum() > 0:\n",
        "            median_value = df_clean[col].median()\n",
        "            df_clean[col] = df_clean[col].fillna(median_value)\n",
        "            print(f\"‚úÖ Filled {col} missing values with median: {median_value:.2f}\")\n",
        "    \n",
        "    # Categorical columns - use mode\n",
        "    categorical_columns = df_clean.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_columns:\n",
        "        if df_clean[col].isnull().sum() > 0:\n",
        "            mode_value = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'\n",
        "            df_clean[col] = df_clean[col].fillna(mode_value)\n",
        "            print(f\"‚úÖ Filled {col} missing values with mode: {mode_value}\")\n",
        "    \n",
        "    # 3.1.2 Handle Duplicates\n",
        "    print(\"\\nüîç Step 2: Handling Duplicates\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    duplicates_before = df_clean.duplicated().sum()\n",
        "    print(f\"Duplicate rows found: {duplicates_before}\")\n",
        "    \n",
        "    if duplicates_before > 0:\n",
        "        # Remove exact duplicates\n",
        "        df_clean = df_clean.drop_duplicates()\n",
        "        print(f\"‚úÖ Removed {duplicates_before} duplicate rows\")\n",
        "        \n",
        "        # Check for duplicates based on specific columns\n",
        "        key_columns = ['customer_id', 'email']\n",
        "        subset_duplicates = df_clean.duplicated(subset=key_columns).sum()\n",
        "        if subset_duplicates > 0:\n",
        "            df_clean = df_clean.drop_duplicates(subset=key_columns, keep='first')\n",
        "            print(f\"‚úÖ Removed {subset_duplicates} duplicates based on {key_columns}\")\n",
        "    else:\n",
        "        print(\"‚úÖ No duplicates found\")\n",
        "    \n",
        "    # 3.1.3 Convert Data Types\n",
        "    print(\"\\nüîç Step 3: Converting Data Types\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Convert credit_score to numeric, handling 'N/A' values\n",
        "    if 'credit_score' in df_clean.columns:\n",
        "        df_clean['credit_score'] = pd.to_numeric(df_clean['credit_score'], errors='coerce')\n",
        "        # Fill any remaining NaN values with median\n",
        "        df_clean['credit_score'] = df_clean['credit_score'].fillna(df_clean['credit_score'].median())\n",
        "        print(\"‚úÖ Converted credit_score to numeric\")\n",
        "    \n",
        "    # Convert date columns\n",
        "    if 'application_date' in df_clean.columns:\n",
        "        df_clean['application_date'] = pd.to_datetime(df_clean['application_date'])\n",
        "        print(\"‚úÖ Converted application_date to datetime\")\n",
        "    \n",
        "    # Convert categorical columns to category type for memory efficiency\n",
        "    categorical_columns = df_clean.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_columns:\n",
        "        if col not in ['email', 'phone', 'address']:  # Keep text columns as object\n",
        "            df_clean[col] = df_clean[col].astype('category')\n",
        "            print(f\"‚úÖ Converted {col} to category\")\n",
        "    \n",
        "    # 3.1.4 Handle Outliers\n",
        "    print(\"\\nüîç Step 4: Handling Outliers\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Define outlier detection function\n",
        "    def detect_outliers_iqr(df, column):\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "        return outliers\n",
        "    \n",
        "    # Check for outliers in numeric columns\n",
        "    outlier_columns = ['age', 'income', 'credit_score', 'loan_amount']\n",
        "    for col in outlier_columns:\n",
        "        if col in df_clean.columns:\n",
        "            outliers = detect_outliers_iqr(df_clean, col)\n",
        "            if len(outliers) > 0:\n",
        "                print(f\"‚ö†Ô∏è  {col}: {len(outliers)} outliers detected\")\n",
        "                # Cap outliers instead of removing them\n",
        "                Q1 = df_clean[col].quantile(0.25)\n",
        "                Q3 = df_clean[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                \n",
        "                df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "                print(f\"‚úÖ Capped {col} outliers to IQR bounds\")\n",
        "            else:\n",
        "                print(f\"‚úÖ {col}: No outliers detected\")\n",
        "    \n",
        "    # 3.1.5 Handle Inconsistent Data\n",
        "    print(\"\\nüîç Step 5: Handling Inconsistent Data\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Fix impossible ages\n",
        "    if 'age' in df_clean.columns:\n",
        "        impossible_ages = df_clean[(df_clean['age'] < 0) | (df_clean['age'] > 120)]\n",
        "        if len(impossible_ages) > 0:\n",
        "            print(f\"‚ö†Ô∏è  Found {len(impossible_ages)} impossible ages\")\n",
        "            # Replace with median age\n",
        "            median_age = df_clean['age'].median()\n",
        "            df_clean.loc[(df_clean['age'] < 0) | (df_clean['age'] > 120), 'age'] = median_age\n",
        "            print(f\"‚úÖ Replaced impossible ages with median: {median_age}\")\n",
        "        else:\n",
        "            print(\"‚úÖ All ages are within reasonable range\")\n",
        "    \n",
        "    # Fix negative income\n",
        "    if 'income' in df_clean.columns:\n",
        "        negative_income = df_clean[df_clean['income'] < 0]\n",
        "        if len(negative_income) > 0:\n",
        "            print(f\"‚ö†Ô∏è  Found {len(negative_income)} negative income values\")\n",
        "            # Replace with median income\n",
        "            median_income = df_clean['income'].median()\n",
        "            df_clean.loc[df_clean['income'] < 0, 'income'] = median_income\n",
        "            print(f\"‚úÖ Replaced negative income with median: {median_income:.2f}\")\n",
        "        else:\n",
        "            print(\"‚úÖ All income values are non-negative\")\n",
        "    \n",
        "    # 3.1.6 Summary\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"üìã CLEANING SUMMARY:\")\n",
        "    print(f\"Original shape: {original_shape}\")\n",
        "    print(f\"Cleaned shape: {df_clean.shape}\")\n",
        "    print(f\"Rows removed: {original_shape[0] - df_clean.shape[0]}\")\n",
        "    print(f\"Columns: {original_shape[1]} (unchanged)\")\n",
        "    \n",
        "    # Check final missing values\n",
        "    missing_after = df_clean.isnull().sum().sum()\n",
        "    print(f\"Missing values after cleaning: {missing_after}\")\n",
        "    \n",
        "    if missing_after == 0:\n",
        "        print(\"üéâ Data cleaning completed successfully!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Some missing values remain - manual review needed\")\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Apply data cleaning\n",
        "df_cleaned = clean_data(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Standardization\n",
        "\n",
        "**Purpose**: Convert data into structured, uniform formats for consistent processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1 Data Standardization Pipeline\n",
        "def standardize_data(df):\n",
        "    \"\"\"\n",
        "    Comprehensive data standardization pipeline\n",
        "    \"\"\"\n",
        "    print(\"üîß Starting Data Standardization...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Create a copy to avoid modifying original data\n",
        "    df_std = df.copy()\n",
        "    \n",
        "    # 4.1.1 Standardize Column Names\n",
        "    print(\"üìù Step 1: Standardizing Column Names\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Convert to lowercase and replace spaces with underscores\n",
        "    original_columns = df_std.columns.tolist()\n",
        "    df_std.columns = df_std.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
        "    \n",
        "    print(\"Column name changes:\")\n",
        "    for orig, new in zip(original_columns, df_std.columns):\n",
        "        if orig != new:\n",
        "            print(f\"  '{orig}' ‚Üí '{new}'\")\n",
        "    \n",
        "    # 4.1.2 Standardize Date Formats\n",
        "    print(\"\\nüìÖ Step 2: Standardizing Date Formats\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    if 'application_date' in df_std.columns:\n",
        "        # Ensure datetime format\n",
        "        df_std['application_date'] = pd.to_datetime(df_std['application_date'], errors='coerce')\n",
        "        print(\"‚úÖ Standardized application_date to datetime format\")\n",
        "        \n",
        "        # Extract date components\n",
        "        df_std['year'] = df_std['application_date'].dt.year\n",
        "        df_std['month'] = df_std['application_date'].dt.month\n",
        "        df_std['day'] = df_std['application_date'].dt.day\n",
        "        df_std['day_of_week'] = df_std['application_date'].dt.dayofweek\n",
        "        df_std['is_weekend'] = df_std['day_of_week'].isin([5, 6])\n",
        "        print(\"‚úÖ Extracted date components\")\n",
        "    \n",
        "    # 4.1.3 Standardize Text Data\n",
        "    print(\"\\nüìù Step 3: Standardizing Text Data\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Standardize email addresses\n",
        "    if 'email' in df_std.columns:\n",
        "        df_std['email'] = df_std['email'].str.lower().str.strip()\n",
        "        print(\"‚úÖ Standardized email addresses (lowercase, trimmed)\")\n",
        "    \n",
        "    # Standardize names and addresses\n",
        "    text_columns = ['city', 'state', 'education', 'employment_status', 'marital_status', 'home_ownership', 'purpose']\n",
        "    for col in text_columns:\n",
        "        if col in df_std.columns:\n",
        "            df_std[col] = df_std[col].str.strip().str.title()\n",
        "            print(f\"‚úÖ Standardized {col} (title case, trimmed)\")\n",
        "    \n",
        "    # 4.1.4 Standardize Categorical Values\n",
        "    print(\"\\nüè∑Ô∏è Step 4: Standardizing Categorical Values\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Standardize employment status\n",
        "    if 'employment_status' in df_std.columns:\n",
        "        employment_mapping = {\n",
        "            'Employed': 'EMPLOYED',\n",
        "            'Unemployed': 'UNEMPLOYED',\n",
        "            'Self-Employed': 'SELF_EMPLOYED',\n",
        "            'Retired': 'RETIRED'\n",
        "        }\n",
        "        df_std['employment_status'] = df_std['employment_status'].map(employment_mapping).fillna(df_std['employment_status'])\n",
        "        print(\"‚úÖ Standardized employment status values\")\n",
        "    \n",
        "    # Standardize education levels\n",
        "    if 'education' in df_std.columns:\n",
        "        education_mapping = {\n",
        "            'High School': 'HIGH_SCHOOL',\n",
        "            'Bachelor': 'BACHELOR',\n",
        "            'Master': 'MASTER',\n",
        "            'PhD': 'PHD',\n",
        "            'Associate': 'ASSOCIATE'\n",
        "        }\n",
        "        df_std['education'] = df_std['education'].map(education_mapping).fillna(df_std['education'])\n",
        "        print(\"‚úÖ Standardized education levels\")\n",
        "    \n",
        "    # 4.1.5 Standardize Numerical Data\n",
        "    print(\"\\nüî¢ Step 5: Standardizing Numerical Data\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Round numerical values to appropriate precision\n",
        "    if 'income' in df_std.columns:\n",
        "        df_std['income'] = df_std['income'].round(2)\n",
        "        print(\"‚úÖ Rounded income to 2 decimal places\")\n",
        "    \n",
        "    if 'loan_amount' in df_std.columns:\n",
        "        df_std['loan_amount'] = df_std['loan_amount'].round(2)\n",
        "        print(\"‚úÖ Rounded loan_amount to 2 decimal places\")\n",
        "    \n",
        "    if 'interest_rate' in df_std.columns:\n",
        "        df_std['interest_rate'] = df_std['interest_rate'].round(2)\n",
        "        print(\"‚úÖ Rounded interest_rate to 2 decimal places\")\n",
        "    \n",
        "    # 4.1.6 Create Standardized Categories\n",
        "    print(\"\\nüìä Step 6: Creating Standardized Categories\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Create age groups\n",
        "    if 'age' in df_std.columns:\n",
        "        df_std['age_group'] = pd.cut(df_std['age'], \n",
        "                                   bins=[0, 25, 35, 45, 55, 65, 100], \n",
        "                                   labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
        "        print(\"‚úÖ Created age groups\")\n",
        "    \n",
        "    # Create income brackets\n",
        "    if 'income' in df_std.columns:\n",
        "        df_std['income_bracket'] = pd.cut(df_std['income'], \n",
        "                                        bins=[0, 30000, 50000, 75000, 100000, float('inf')], \n",
        "                                        labels=['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High'])\n",
        "        print(\"‚úÖ Created income brackets\")\n",
        "    \n",
        "    # Create credit score categories\n",
        "    if 'credit_score' in df_std.columns:\n",
        "        df_std['credit_rating'] = pd.cut(df_std['credit_score'], \n",
        "                                       bins=[0, 580, 670, 740, 800, 850], \n",
        "                                       labels=['Poor', 'Fair', 'Good', 'Very Good', 'Excellent'])\n",
        "        print(\"‚úÖ Created credit score categories\")\n",
        "    \n",
        "    # 4.1.7 Summary\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"üìã STANDARDIZATION SUMMARY:\")\n",
        "    print(f\"Original columns: {len(original_columns)}\")\n",
        "    print(f\"Standardized columns: {len(df_std.columns)}\")\n",
        "    print(f\"New columns added: {len(df_std.columns) - len(original_columns)}\")\n",
        "    print(\"‚úÖ Data standardization completed successfully!\")\n",
        "    \n",
        "    return df_std\n",
        "\n",
        "# Apply data standardization\n",
        "df_standardized = standardize_data(df_cleaned)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Transformation\n",
        "\n",
        "**Purpose**: Scale, normalize, and encode data for machine learning algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.1 Data Transformation Pipeline\n",
        "def transform_data(df):\n",
        "    \"\"\"\n",
        "    Comprehensive data transformation pipeline\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Starting Data Transformation...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Create a copy to avoid modifying original data\n",
        "    df_transformed = df.copy()\n",
        "    \n",
        "    # 5.1.1 Feature Scaling\n",
        "    print(\"üìè Step 1: Feature Scaling\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Identify numerical columns for scaling\n",
        "    numerical_columns = df_transformed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    # Remove target variable and ID columns\n",
        "    exclude_columns = ['customer_id', 'default_risk', 'year', 'month', 'day', 'day_of_week']\n",
        "    scaling_columns = [col for col in numerical_columns if col not in exclude_columns]\n",
        "    \n",
        "    print(f\"Columns to be scaled: {scaling_columns}\")\n",
        "    \n",
        "    # Apply different scaling methods\n",
        "    scalers = {}\n",
        "    \n",
        "    # Standard Scaling (mean=0, std=1)\n",
        "    standard_scaler = StandardScaler()\n",
        "    df_transformed[[f'{col}_standard' for col in scaling_columns]] = standard_scaler.fit_transform(df_transformed[scaling_columns])\n",
        "    scalers['standard'] = standard_scaler\n",
        "    print(\"‚úÖ Applied Standard Scaling\")\n",
        "    \n",
        "    # Min-Max Scaling (0-1 range)\n",
        "    minmax_scaler = MinMaxScaler()\n",
        "    df_transformed[[f'{col}_minmax' for col in scaling_columns]] = minmax_scaler.fit_transform(df_transformed[scaling_columns])\n",
        "    scalers['minmax'] = minmax_scaler\n",
        "    print(\"‚úÖ Applied Min-Max Scaling\")\n",
        "    \n",
        "    # 5.1.2 Categorical Encoding\n",
        "    print(\"\\nüè∑Ô∏è Step 2: Categorical Encoding\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Identify categorical columns\n",
        "    categorical_columns = df_transformed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    # Exclude text columns that shouldn't be encoded\n",
        "    exclude_text_columns = ['email', 'phone', 'address']\n",
        "    encoding_columns = [col for col in categorical_columns if col not in exclude_text_columns]\n",
        "    \n",
        "    print(f\"Columns to be encoded: {encoding_columns}\")\n",
        "    \n",
        "    # One-hot encoding for low cardinality categoricals\n",
        "    low_cardinality_cols = []\n",
        "    for col in encoding_columns:\n",
        "        if df_transformed[col].nunique() <= 10:  # Low cardinality threshold\n",
        "            low_cardinality_cols.append(col)\n",
        "    \n",
        "    if low_cardinality_cols:\n",
        "        df_encoded = pd.get_dummies(df_transformed, columns=low_cardinality_cols, prefix=low_cardinality_cols)\n",
        "        df_transformed = df_encoded\n",
        "        print(f\"‚úÖ Applied One-hot encoding to: {low_cardinality_cols}\")\n",
        "    \n",
        "    # Label encoding for high cardinality categoricals\n",
        "    high_cardinality_cols = [col for col in encoding_columns if col not in low_cardinality_cols]\n",
        "    label_encoders = {}\n",
        "    \n",
        "    for col in high_cardinality_cols:\n",
        "        if col in df_transformed.columns:\n",
        "            le = LabelEncoder()\n",
        "            df_transformed[f'{col}_encoded'] = le.fit_transform(df_transformed[col].astype(str))\n",
        "            label_encoders[col] = le\n",
        "            print(f\"‚úÖ Applied Label encoding to: {col}\")\n",
        "    \n",
        "    # 5.1.3 Feature Creation\n",
        "    print(\"\\nüîß Step 3: Feature Creation\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Create derived features\n",
        "    if 'income' in df_transformed.columns and 'loan_amount' in df_transformed.columns:\n",
        "        df_transformed['loan_to_income_ratio'] = df_transformed['loan_amount'] / df_transformed['income']\n",
        "        print(\"‚úÖ Created loan_to_income_ratio\")\n",
        "    \n",
        "    if 'age' in df_transformed.columns and 'employment_length_years' in df_transformed.columns:\n",
        "        df_transformed['employment_age_ratio'] = df_transformed['employment_length_years'] / df_transformed['age']\n",
        "        print(\"‚úÖ Created employment_age_ratio\")\n",
        "    \n",
        "    if 'credit_score' in df_transformed.columns and 'late_payments' in df_transformed.columns:\n",
        "        df_transformed['credit_risk_score'] = df_transformed['credit_score'] - (df_transformed['late_payments'] * 10)\n",
        "        print(\"‚úÖ Created credit_risk_score\")\n",
        "    \n",
        "    # Create interaction features\n",
        "    if 'age' in df_transformed.columns and 'income' in df_transformed.columns:\n",
        "        df_transformed['age_income_interaction'] = df_transformed['age'] * df_transformed['income']\n",
        "        print(\"‚úÖ Created age_income_interaction\")\n",
        "    \n",
        "    # 5.1.4 Polynomial Features\n",
        "    print(\"\\nüìà Step 4: Polynomial Features\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    from sklearn.preprocessing import PolynomialFeatures\n",
        "    \n",
        "    # Select key numerical features for polynomial expansion\n",
        "    poly_features = ['age', 'income', 'credit_score']\n",
        "    poly_features = [col for col in poly_features if col in df_transformed.columns]\n",
        "    \n",
        "    if poly_features:\n",
        "        poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
        "        poly_data = poly.fit_transform(df_transformed[poly_features])\n",
        "        poly_columns = poly.get_feature_names_out(poly_features)\n",
        "        \n",
        "        # Add polynomial features to dataframe\n",
        "        for i, col in enumerate(poly_columns):\n",
        "            if col not in df_transformed.columns:  # Avoid duplicates\n",
        "                df_transformed[f'poly_{col}'] = poly_data[:, i]\n",
        "        \n",
        "        print(f\"‚úÖ Created polynomial features for: {poly_features}\")\n",
        "    \n",
        "    # 5.1.5 Feature Selection\n",
        "    print(\"\\nüéØ Step 5: Feature Selection\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Remove highly correlated features\n",
        "    numerical_cols = df_transformed.select_dtypes(include=[np.number]).columns\n",
        "    correlation_matrix = df_transformed[numerical_cols].corr().abs()\n",
        "    \n",
        "    # Find highly correlated pairs\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i+1, len(correlation_matrix.columns)):\n",
        "            if correlation_matrix.iloc[i, j] > 0.95:  # High correlation threshold\n",
        "                high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j]))\n",
        "    \n",
        "    # Remove one feature from each highly correlated pair\n",
        "    features_to_remove = set()\n",
        "    for feat1, feat2 in high_corr_pairs:\n",
        "        if feat1 not in features_to_remove:\n",
        "            features_to_remove.add(feat2)\n",
        "    \n",
        "    if features_to_remove:\n",
        "        df_transformed = df_transformed.drop(columns=list(features_to_remove))\n",
        "        print(f\"‚úÖ Removed highly correlated features: {list(features_to_remove)}\")\n",
        "    else:\n",
        "        print(\"‚úÖ No highly correlated features found\")\n",
        "    \n",
        "    # 5.1.6 Summary\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"üìã TRANSFORMATION SUMMARY:\")\n",
        "    print(f\"Original shape: {df.shape}\")\n",
        "    print(f\"Transformed shape: {df_transformed.shape}\")\n",
        "    print(f\"Features added: {df_transformed.shape[1] - df.shape[1]}\")\n",
        "    print(f\"Scalers created: {len(scalers)}\")\n",
        "    print(f\"Label encoders created: {len(label_encoders)}\")\n",
        "    print(\"‚úÖ Data transformation completed successfully!\")\n",
        "    \n",
        "    return df_transformed, scalers, label_encoders\n",
        "\n",
        "# Apply data transformation\n",
        "df_transformed, scalers, label_encoders = transform_data(df_standardized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Curation\n",
        "\n",
        "**Purpose**: Organize datasets for efficient feature engineering and model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6.1 Data Curation Pipeline\n",
        "def curate_data(df, scalers, label_encoders):\n",
        "    \"\"\"\n",
        "    Comprehensive data curation pipeline\n",
        "    \"\"\"\n",
        "    print(\"üìö Starting Data Curation...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Create a copy to avoid modifying original data\n",
        "    df_curated = df.copy()\n",
        "    \n",
        "    # 6.1.1 Create Data Versioning\n",
        "    print(\"üî¢ Step 1: Creating Data Versioning\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    import hashlib\n",
        "    import json\n",
        "    import os\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Create data hash for versioning\n",
        "    data_string = df_curated.to_string()\n",
        "    data_hash = hashlib.md5(data_string.encode()).hexdigest()\n",
        "    version_id = data_hash[:8]\n",
        "    \n",
        "    print(f\"Data version ID: {version_id}\")\n",
        "    \n",
        "    # 6.1.2 Create Data Profile\n",
        "    print(\"\\nüìä Step 2: Creating Data Profile\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Generate comprehensive data profile\n",
        "    profile = {\n",
        "        'version_id': version_id,\n",
        "        'created_at': datetime.now().isoformat(),\n",
        "        'shape': df_curated.shape,\n",
        "        'columns': list(df_curated.columns),\n",
        "        'dtypes': df_curated.dtypes.to_dict(),\n",
        "        'null_counts': df_curated.isnull().sum().to_dict(),\n",
        "        'unique_counts': df_curated.nunique().to_dict(),\n",
        "        'memory_usage': df_curated.memory_usage(deep=True).sum(),\n",
        "        'statistical_summary': df_curated.describe().to_dict()\n",
        "    }\n",
        "    \n",
        "    print(\"‚úÖ Data profile created\")\n",
        "    \n",
        "    # 6.1.3 Create Train/Validation/Test Splits\n",
        "    print(\"\\n‚úÇÔ∏è Step 3: Creating Data Splits\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Prepare features and target\n",
        "    feature_columns = [col for col in df_curated.columns if col not in ['customer_id', 'default_risk']]\n",
        "    X = df_curated[feature_columns]\n",
        "    y = df_curated['default_risk']\n",
        "    \n",
        "    # Create splits\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "    \n",
        "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "    print(f\"Features: {X_train.shape[1]}\")\n",
        "    \n",
        "    # 6.1.4 Save Curated Data\n",
        "    print(\"\\nüíæ Step 4: Saving Curated Data\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Create directories\n",
        "    os.makedirs('data/processed', exist_ok=True)\n",
        "    os.makedirs('data/raw', exist_ok=True)\n",
        "    \n",
        "    # Save datasets\n",
        "    df_curated.to_parquet(f'data/processed/dataset_v{version_id}.parquet', index=False)\n",
        "    X_train.to_parquet(f'data/processed/X_train_v{version_id}.parquet', index=False)\n",
        "    X_val.to_parquet(f'data/processed/X_val_v{version_id}.parquet', index=False)\n",
        "    X_test.to_parquet(f'data/processed/X_test_v{version_id}.parquet', index=False)\n",
        "    y_train.to_parquet(f'data/processed/y_train_v{version_id}.parquet', index=False)\n",
        "    y_val.to_parquet(f'data/processed/y_val_v{version_id}.parquet', index=False)\n",
        "    y_test.to_parquet(f'data/processed/y_test_v{version_id}.parquet', index=False)\n",
        "    \n",
        "    print(\"‚úÖ Datasets saved in Parquet format\")\n",
        "    \n",
        "    # 6.1.5 Save Metadata and Artifacts\n",
        "    print(\"\\nüìã Step 5: Saving Metadata and Artifacts\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Save data profile\n",
        "    with open(f'data/processed/metadata_v{version_id}.json', 'w') as f:\n",
        "        json.dump(profile, f, indent=2, default=str)\n",
        "    \n",
        "    # Save scalers\n",
        "    import joblib\n",
        "    joblib.dump(scalers, f'data/processed/scalers_v{version_id}.joblib')\n",
        "    \n",
        "    # Save label encoders\n",
        "    joblib.dump(label_encoders, f'data/processed/label_encoders_v{version_id}.joblib')\n",
        "    \n",
        "    # Save feature names\n",
        "    feature_info = {\n",
        "        'feature_names': list(X_train.columns),\n",
        "        'target_name': 'default_risk',\n",
        "        'categorical_features': list(df_curated.select_dtypes(include=['object', 'category']).columns),\n",
        "        'numerical_features': list(df_curated.select_dtypes(include=[np.number]).columns)\n",
        "    }\n",
        "    \n",
        "    with open(f'data/processed/feature_info_v{version_id}.json', 'w') as f:\n",
        "        json.dump(feature_info, f, indent=2)\n",
        "    \n",
        "    print(\"‚úÖ Metadata and artifacts saved\")\n",
        "    \n",
        "    # 6.1.6 Create Data Dictionary\n",
        "    print(\"\\nüìñ Step 6: Creating Data Dictionary\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    data_dictionary = []\n",
        "    for col in df_curated.columns:\n",
        "        col_info = {\n",
        "            'column_name': col,\n",
        "            'data_type': str(df_curated[col].dtype),\n",
        "            'null_count': df_curated[col].isnull().sum(),\n",
        "            'null_percentage': (df_curated[col].isnull().sum() / len(df_curated)) * 100,\n",
        "            'unique_count': df_curated[col].nunique(),\n",
        "            'description': f\"Feature: {col}\"\n",
        "        }\n",
        "        \n",
        "        if df_curated[col].dtype in ['int64', 'float64']:\n",
        "            col_info.update({\n",
        "                'min_value': df_curated[col].min(),\n",
        "                'max_value': df_curated[col].max(),\n",
        "                'mean_value': df_curated[col].mean(),\n",
        "                'std_value': df_curated[col].std()\n",
        "            })\n",
        "        else:\n",
        "            col_info['top_values'] = df_curated[col].value_counts().head(5).to_dict()\n",
        "        \n",
        "        data_dictionary.append(col_info)\n",
        "    \n",
        "    # Save data dictionary\n",
        "    data_dict_df = pd.DataFrame(data_dictionary)\n",
        "    data_dict_df.to_csv(f'data/processed/data_dictionary_v{version_id}.csv', index=False)\n",
        "    \n",
        "    print(\"‚úÖ Data dictionary created\")\n",
        "    \n",
        "    # 6.1.7 Summary\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"üìã CURATION SUMMARY:\")\n",
        "    print(f\"Version ID: {version_id}\")\n",
        "    print(f\"Total samples: {df_curated.shape[0]}\")\n",
        "    print(f\"Total features: {df_curated.shape[1]}\")\n",
        "    print(f\"Training samples: {X_train.shape[0]}\")\n",
        "    print(f\"Validation samples: {X_val.shape[0]}\")\n",
        "    print(f\"Test samples: {X_test.shape[0]}\")\n",
        "    print(f\"Memory usage: {profile['memory_usage'] / 1024**2:.2f} MB\")\n",
        "    print(\"‚úÖ Data curation completed successfully!\")\n",
        "    \n",
        "    return {\n",
        "        'version_id': version_id,\n",
        "        'df_curated': df_curated,\n",
        "        'X_train': X_train,\n",
        "        'X_val': X_val,\n",
        "        'X_test': X_test,\n",
        "        'y_train': y_train,\n",
        "        'y_val': y_val,\n",
        "        'y_test': y_test,\n",
        "        'profile': profile,\n",
        "        'feature_info': feature_info\n",
        "    }\n",
        "\n",
        "# Apply data curation\n",
        "curation_results = curate_data(df_transformed, scalers, label_encoders)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
