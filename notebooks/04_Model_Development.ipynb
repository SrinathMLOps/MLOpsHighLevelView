{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Phase 4: Model Development\n",
        "\n",
        "This notebook demonstrates comprehensive model development techniques for MLOps, covering model selection, training, validation, and evaluation.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Model Selection](#1-model-selection)\n",
        "2. [Model Training](#2-model-training)\n",
        "3. [Model Validation](#3-model-validation)\n",
        "4. [Model Evaluation](#4-model-evaluation)\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "Make sure you have the required libraries installed:\n",
        "```bash\n",
        "pip install pandas numpy scikit-learn matplotlib seaborn plotly xgboost lightgbm\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "\n",
        "# Load data (create sample if not available)\n",
        "try:\n",
        "    df = pd.read_parquet('data/processed/dataset_v*.parquet')\n",
        "    print(\"‚úÖ Loaded processed data\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  Creating sample data...\")\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1000\n",
        "    \n",
        "    data = {\n",
        "        'customer_id': range(1, n_samples + 1),\n",
        "        'age': np.random.normal(35, 12, n_samples).astype(int),\n",
        "        'income': np.random.lognormal(10, 0.5, n_samples),\n",
        "        'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples, p=[0.3, 0.4, 0.2, 0.1]),\n",
        "        'employment_status': np.random.choice(['Employed', 'Unemployed', 'Self-employed', 'Retired'], n_samples, p=[0.6, 0.1, 0.2, 0.1]),\n",
        "        'credit_score': np.random.normal(650, 100, n_samples).astype(int),\n",
        "        'loan_amount': np.random.exponential(50000, n_samples),\n",
        "        'default_risk': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
        "        'application_date': pd.date_range('2020-01-01', periods=n_samples, freq='D'),\n",
        "        'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_samples, p=[0.2, 0.15, 0.15, 0.15, 0.35]),\n",
        "        'marital_status': np.random.choice(['Single', 'Married', 'Divorced', 'Widowed'], n_samples, p=[0.4, 0.4, 0.15, 0.05]),\n",
        "        'dependents': np.random.poisson(1.5, n_samples),\n",
        "        'previous_loans': np.random.poisson(2, n_samples),\n",
        "        'late_payments': np.random.poisson(0.5, n_samples),\n",
        "        'debt_to_income_ratio': np.random.beta(2, 5, n_samples),\n",
        "        'credit_utilization': np.random.beta(3, 2, n_samples),\n",
        "        'home_ownership': np.random.choice(['Rent', 'Own', 'Mortgage'], n_samples, p=[0.4, 0.2, 0.4]),\n",
        "        'purpose': np.random.choice(['Debt Consolidation', 'Home Improvement', 'Business', 'Education'], n_samples, p=[0.4, 0.2, 0.2, 0.2])\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"‚úÖ Sample dataset created\")\n",
        "\n",
        "print(f\"üìä Dataset shape: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Model Selection\n",
        "\n",
        "**Purpose**: Identify and compare different machine learning algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1 Prepare Data for Modeling\n",
        "print(\"ü§ñ Step 11: Identify Candidate Models\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Prepare features and target\n",
        "X = df.drop(['customer_id', 'default_risk', 'application_date'], axis=1, errors='ignore')\n",
        "y = df['default_risk']\n",
        "\n",
        "# Handle categorical variables\n",
        "X_encoded = pd.get_dummies(X, drop_first=True)\n",
        "print(f\"Features shape: {X_encoded.shape}\")\n",
        "print(f\"Target distribution: {y.value_counts()}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "\n",
        "# 1.2 Define Candidate Models\n",
        "print(\"\\nüéØ Candidate Models\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42, probability=True),\n",
        "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
        "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
        "}\n",
        "\n",
        "print(\"Models to evaluate:\")\n",
        "for name in models.keys():\n",
        "    print(f\"  - {name}\")\n",
        "\n",
        "# 1.3 Initial Model Evaluation\n",
        "print(\"\\nüìä Initial Model Performance\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "model_scores = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nEvaluating {name}...\")\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    \n",
        "    # Train and test\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "    \n",
        "    model_scores[name] = {\n",
        "        'cv_mean': cv_scores.mean(),\n",
        "        'cv_std': cv_scores.std(),\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc\n",
        "    }\n",
        "    \n",
        "    print(f\"  CV Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
        "    print(f\"  Test Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"  Test F1-Score: {f1:.3f}\")\n",
        "    print(f\"  Test ROC-AUC: {roc_auc:.3f}\")\n",
        "\n",
        "# 1.4 Model Comparison\n",
        "print(\"\\nüìà Model Comparison Summary\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "results_df = pd.DataFrame(model_scores).T\n",
        "results_df = results_df.sort_values('roc_auc', ascending=False)\n",
        "\n",
        "print(results_df[['cv_mean', 'accuracy', 'f1', 'roc_auc']].round(3))\n",
        "\n",
        "# Visualize model performance\n",
        "plt.figure(figsize=(12, 8))\n",
        "metrics = ['cv_mean', 'accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.12\n",
        "\n",
        "for i, (model_name, scores) in enumerate(model_scores.items()):\n",
        "    plt.bar(x + i * width, [scores[metric] for metric in metrics], \n",
        "            width, label=model_name, alpha=0.8)\n",
        "\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xticks(x + width * 2.5, metrics)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Model selection completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training\n",
        "\n",
        "**Purpose**: Train models with optimized hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1 Hyperparameter Tuning\n",
        "print(\"üéØ Step 12: Write Training Code\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Select best performing models for hyperparameter tuning\n",
        "best_models = ['Random Forest', 'XGBoost', 'LightGBM']\n",
        "\n",
        "# Define parameter grids\n",
        "param_grids = {\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [3, 6, 9],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'subsample': [0.8, 0.9, 1.0]\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [3, 6, 9],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'num_leaves': [31, 50, 100]\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Hyperparameter tuning for best models...\")\n",
        "\n",
        "# 2.2 Grid Search for Best Models\n",
        "tuned_models = {}\n",
        "tuning_results = {}\n",
        "\n",
        "for model_name in best_models:\n",
        "    print(f\"\\nüîß Tuning {model_name}...\")\n",
        "    \n",
        "    model = models[model_name]\n",
        "    param_grid = param_grids[model_name]\n",
        "    \n",
        "    # Use RandomizedSearchCV for efficiency\n",
        "    grid_search = RandomizedSearchCV(\n",
        "        model, \n",
        "        param_grid, \n",
        "        n_iter=20,  # Reduced for demo\n",
        "        cv=3, \n",
        "        scoring='roc_auc',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    grid_search.fit(X_train, y_train)\n",
        "    \n",
        "    tuned_models[model_name] = grid_search.best_estimator_\n",
        "    tuning_results[model_name] = {\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'best_score': grid_search.best_score_\n",
        "    }\n",
        "    \n",
        "    print(f\"  Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"  Best CV score: {grid_search.best_score_:.3f}\")\n",
        "\n",
        "# 2.3 Train Final Models\n",
        "print(\"\\nüöÄ Step 13: Train Models\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "final_models = {}\n",
        "\n",
        "for model_name, model in tuned_models.items():\n",
        "    print(f\"Training final {model_name}...\")\n",
        "    \n",
        "    # Train on full training set\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "    test_precision = precision_score(y_test, y_pred_test)\n",
        "    test_recall = recall_score(y_test, y_pred_test)\n",
        "    test_f1 = f1_score(y_test, y_pred_test)\n",
        "    test_roc_auc = roc_auc_score(y_test, y_pred_proba_test)\n",
        "    \n",
        "    final_models[model_name] = {\n",
        "        'model': model,\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'test_precision': test_precision,\n",
        "        'test_recall': test_recall,\n",
        "        'test_f1': test_f1,\n",
        "        'test_roc_auc': test_roc_auc,\n",
        "        'predictions': y_pred_test,\n",
        "        'probabilities': y_pred_proba_test\n",
        "    }\n",
        "    \n",
        "    print(f\"  Train Accuracy: {train_accuracy:.3f}\")\n",
        "    print(f\"  Test Accuracy: {test_accuracy:.3f}\")\n",
        "    print(f\"  Test F1-Score: {test_f1:.3f}\")\n",
        "    print(f\"  Test ROC-AUC: {test_roc_auc:.3f}\")\n",
        "\n",
        "print(\"‚úÖ Model training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Validation\n",
        "\n",
        "**Purpose**: Validate model performance using various techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1 Cross-Validation Analysis\n",
        "print(\"üîç Step 14: Validate & Evaluate Models\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Cross-validation for all final models\n",
        "cv_results = {}\n",
        "\n",
        "for model_name, model_info in final_models.items():\n",
        "    print(f\"\\nCross-validating {model_name}...\")\n",
        "    \n",
        "    model = model_info['model']\n",
        "    \n",
        "    # 5-fold cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "    \n",
        "    cv_results[model_name] = {\n",
        "        'cv_mean': cv_scores.mean(),\n",
        "        'cv_std': cv_scores.std(),\n",
        "        'cv_scores': cv_scores\n",
        "    }\n",
        "    \n",
        "    print(f\"  CV ROC-AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
        "\n",
        "# 3.2 Learning Curves\n",
        "print(\"\\nüìà Learning Curves\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "def plot_learning_curve(model, X, y, title):\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        model, X, y, cv=3, n_jobs=-1,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "        scoring='roc_auc'\n",
        "    )\n",
        "    \n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    val_mean = np.mean(val_scores, axis=1)\n",
        "    val_std = np.std(val_scores, axis=1)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
        "    plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')\n",
        "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
        "    \n",
        "    plt.title(f'Learning Curve - {title}')\n",
        "    plt.xlabel('Training Set Size')\n",
        "    plt.ylabel('ROC-AUC Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# Plot learning curves for best models\n",
        "for model_name in ['Random Forest', 'XGBoost']:\n",
        "    if model_name in final_models:\n",
        "        plot_learning_curve(final_models[model_name]['model'], X_train, y_train, model_name)\n",
        "\n",
        "# 3.3 Validation Summary\n",
        "print(\"\\nüìä Validation Summary\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "validation_summary = pd.DataFrame({\n",
        "    'Model': list(cv_results.keys()),\n",
        "    'CV_Mean': [cv_results[name]['cv_mean'] for name in cv_results.keys()],\n",
        "    'CV_Std': [cv_results[name]['cv_std'] for name in cv_results.keys()],\n",
        "    'Test_ROC_AUC': [final_models[name]['test_roc_auc'] for name in cv_results.keys()]\n",
        "}).sort_values('Test_ROC_AUC', ascending=False)\n",
        "\n",
        "print(validation_summary.round(3))\n",
        "\n",
        "print(\"‚úÖ Model validation completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation\n",
        "\n",
        "**Purpose**: Comprehensive evaluation with detailed metrics and visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1 Detailed Performance Metrics\n",
        "print(\"üìä Detailed Model Evaluation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Select best model based on ROC-AUC\n",
        "best_model_name = max(final_models.keys(), key=lambda x: final_models[x]['test_roc_auc'])\n",
        "best_model_info = final_models[best_model_name]\n",
        "\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"Test ROC-AUC: {best_model_info['test_roc_auc']:.3f}\")\n",
        "\n",
        "# Detailed metrics for best model\n",
        "y_pred_best = best_model_info['predictions']\n",
        "y_proba_best = best_model_info['probabilities']\n",
        "\n",
        "print(f\"\\nDetailed Metrics for {best_model_name}:\")\n",
        "print(f\"Accuracy: {best_model_info['test_accuracy']:.3f}\")\n",
        "print(f\"Precision: {best_model_info['test_precision']:.3f}\")\n",
        "print(f\"Recall: {best_model_info['test_recall']:.3f}\")\n",
        "print(f\"F1-Score: {best_model_info['test_f1']:.3f}\")\n",
        "print(f\"ROC-AUC: {best_model_info['test_roc_auc']:.3f}\")\n",
        "\n",
        "# 4.2 Confusion Matrix\n",
        "print(\"\\nüîç Confusion Matrix\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['No Default', 'Default'],\n",
        "            yticklabels=['No Default', 'Default'])\n",
        "plt.title(f'Confusion Matrix - {best_model_name}')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# 4.3 ROC Curve\n",
        "print(\"\\nüìà ROC Curve\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba_best)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
        "         label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title(f'ROC Curve - {best_model_name}')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 4.4 Feature Importance (for tree-based models)\n",
        "if best_model_name in ['Random Forest', 'XGBoost', 'LightGBM']:\n",
        "    print(f\"\\nüéØ Feature Importance - {best_model_name}\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    if hasattr(best_model_info['model'], 'feature_importances_'):\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': X_encoded.columns,\n",
        "            'importance': best_model_info['model'].feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        print(\"Top 10 Most Important Features:\")\n",
        "        print(feature_importance.head(10))\n",
        "        \n",
        "        # Plot feature importance\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        top_features = feature_importance.head(15)\n",
        "        plt.barh(range(len(top_features)), top_features['importance'])\n",
        "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "        plt.xlabel('Feature Importance')\n",
        "        plt.title(f'Feature Importance - {best_model_name}')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# 4.5 Model Comparison Visualization\n",
        "print(\"\\nüìä Model Comparison\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Create comparison plot\n",
        "models_comparison = pd.DataFrame({\n",
        "    'Model': list(final_models.keys()),\n",
        "    'Accuracy': [final_models[name]['test_accuracy'] for name in final_models.keys()],\n",
        "    'Precision': [final_models[name]['test_precision'] for name in final_models.keys()],\n",
        "    'Recall': [final_models[name]['test_recall'] for name in final_models.keys()],\n",
        "    'F1-Score': [final_models[name]['test_f1'] for name in final_models.keys()],\n",
        "    'ROC-AUC': [final_models[name]['test_roc_auc'] for name in final_models.keys()]\n",
        "}).sort_values('ROC-AUC', ascending=False)\n",
        "\n",
        "print(models_comparison.round(3))\n",
        "\n",
        "# Save best model\n",
        "best_model = best_model_info['model']\n",
        "joblib.dump(best_model, f'models/best_model_{best_model_name.lower().replace(\" \", \"_\")}.joblib')\n",
        "print(f\"\\n‚úÖ Best model saved as: models/best_model_{best_model_name.lower().replace(' ', '_')}.joblib\")\n",
        "\n",
        "print(\"‚úÖ Model evaluation completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
