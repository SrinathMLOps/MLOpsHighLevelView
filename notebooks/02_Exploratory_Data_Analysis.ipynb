{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Phase 2: Exploratory Data Analysis (EDA)\n",
        "\n",
        "This notebook demonstrates comprehensive exploratory data analysis techniques for MLOps, covering data exploration, selection, filtering, and visualization.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Exploratory Data Analysis](#1-exploratory-data-analysis)\n",
        "2. [Data Selection & Filtering](#2-data-selection--filtering)\n",
        "3. [Data Visualization](#3-data-visualization)\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "Make sure you have the required libraries installed:\n",
        "```bash\n",
        "pip install pandas numpy matplotlib seaborn plotly scipy scikit-learn\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Exploratory Data Analysis\n",
        "\n",
        "**Purpose**: Understand data characteristics, patterns, and potential issues before modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1 Basic Data Summary\n",
        "print(\"üîç Step 7: Exploratory Data Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Basic information about the dataset\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nData Types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Statistical summary\n",
        "print(\"\\nStatistical Summary:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Memory usage\n",
        "print(f\"\\nMemory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Missing values analysis\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percent = (missing_data / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_data,\n",
        "    'Missing Percentage': missing_percent\n",
        "})\n",
        "print(\"\\nMissing Values Analysis:\")\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "# Data quality summary\n",
        "print(f\"\\nData Quality Summary:\")\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "print(f\"Total columns: {len(df.columns)}\")\n",
        "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.2 Distribution Analysis\n",
        "print(\"\\nüìä Distribution Analysis\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Numerical columns distribution\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "print(f\"Numerical columns: {list(numerical_cols)}\")\n",
        "\n",
        "# Create distribution plots for numerical columns\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, col in enumerate(numerical_cols[:6]):\n",
        "    if i < len(axes):\n",
        "        df[col].hist(bins=30, ax=axes[i], alpha=0.7, edgecolor='black')\n",
        "        axes[i].set_title(f'Distribution of {col}')\n",
        "        axes[i].set_xlabel(col)\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Categorical columns analysis\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "print(f\"\\nCategorical columns: {list(categorical_cols)}\")\n",
        "\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\n{col} value counts:\")\n",
        "    print(df[col].value_counts().head(10))\n",
        "    print(f\"Unique values: {df[col].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.3 Correlation Analysis\n",
        "print(\"\\nüîó Correlation Analysis\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Correlation matrix\n",
        "correlation_matrix = df[numerical_cols].corr()\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Pairwise correlations\n",
        "high_corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_val = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr_val) > 0.7:  # High correlation threshold\n",
        "            high_corr_pairs.append((\n",
        "                correlation_matrix.columns[i],\n",
        "                correlation_matrix.columns[j],\n",
        "                corr_val\n",
        "            ))\n",
        "\n",
        "print(\"High Correlation Pairs:\")\n",
        "for pair in high_corr_pairs:\n",
        "    print(f\"{pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
        "\n",
        "# 1.4 Outlier Detection\n",
        "print(\"\\nüîç Outlier Detection\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "def detect_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "    return outliers\n",
        "\n",
        "# Detect outliers in numerical columns\n",
        "outlier_summary = {}\n",
        "for col in numerical_cols:\n",
        "    outliers = detect_outliers_iqr(df, col)\n",
        "    outlier_summary[col] = {\n",
        "        'count': len(outliers),\n",
        "        'percentage': (len(outliers) / len(df)) * 100\n",
        "    }\n",
        "\n",
        "print(\"Outlier Summary:\")\n",
        "for col, info in outlier_summary.items():\n",
        "    print(f\"{col}: {info['count']} outliers ({info['percentage']:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Selection & Filtering\n",
        "\n",
        "**Purpose**: Create targeted datasets for specific analysis and model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1 Data Selection Techniques\n",
        "print(\"‚úÇÔ∏è Step 8: Data Selection & Filtering\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Select specific columns\n",
        "selected_columns = ['age', 'income', 'education', 'default_risk']\n",
        "df_selected = df[selected_columns]\n",
        "print(f\"Selected columns: {selected_columns}\")\n",
        "print(f\"Selected dataset shape: {df_selected.shape}\")\n",
        "\n",
        "# Select rows based on conditions\n",
        "df_filtered = df[df['age'] > 18]\n",
        "print(f\"\\nFiltered by age > 18: {df_filtered.shape[0]} rows\")\n",
        "\n",
        "df_high_income = df[df['income'] > df['income'].quantile(0.8)]\n",
        "print(f\"High income (top 20%): {df_high_income.shape[0]} rows\")\n",
        "\n",
        "# Multiple conditions\n",
        "df_filtered = df[(df['age'] > 18) & (df['income'] > 50000) & (df['education'] == 'Bachelor')]\n",
        "print(f\"Age > 18 AND income > 50k AND Bachelor's: {df_filtered.shape[0]} rows\")\n",
        "\n",
        "# Using .loc[] for label-based selection\n",
        "df_subset = df.loc[df['employment_status'] == 'Employed', ['age', 'income', 'credit_score']]\n",
        "print(f\"Employed customers subset: {df_subset.shape}\")\n",
        "\n",
        "# Using .iloc[] for position-based selection\n",
        "df_subset = df.iloc[0:100, 0:5]  # First 100 rows, first 5 columns\n",
        "print(f\"First 100 rows, first 5 columns: {df_subset.shape}\")\n",
        "\n",
        "# 2.2 Advanced Filtering\n",
        "print(\"\\nüîç Advanced Filtering Techniques\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Filter by data type\n",
        "numerical_data = df.select_dtypes(include=[np.number])\n",
        "categorical_data = df.select_dtypes(include=['object', 'category'])\n",
        "print(f\"Numerical data shape: {numerical_data.shape}\")\n",
        "print(f\"Categorical data shape: {categorical_data.shape}\")\n",
        "\n",
        "# Filter by null values\n",
        "df_no_nulls = df.dropna()\n",
        "print(f\"Rows with no nulls: {df_no_nulls.shape[0]}\")\n",
        "\n",
        "# Filter by string patterns\n",
        "if 'city' in df.columns:\n",
        "    df_ny_customers = df[df['city'].str.contains('New York', na=False)]\n",
        "    print(f\"New York customers: {df_ny_customers.shape[0]}\")\n",
        "\n",
        "# Filter by date ranges (if date column exists)\n",
        "if 'application_date' in df.columns:\n",
        "    df_recent = df[df['application_date'] >= '2020-06-01']\n",
        "    print(f\"Recent applications (after June 2020): {df_recent.shape[0]}\")\n",
        "\n",
        "# Filter by quantiles\n",
        "df_top_10_percent = df[df['income'] >= df['income'].quantile(0.9)]\n",
        "print(f\"Top 10% income: {df_top_10_percent.shape[0]} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Visualization\n",
        "\n",
        "**Purpose**: Create visual representations to understand data patterns and relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1 Basic Plotting with Matplotlib\n",
        "print(\"üìä Step 9: Data Visualization\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Line plots for time series (if date column exists)\n",
        "if 'application_date' in df.columns:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    daily_counts = df.groupby(df['application_date'].dt.date).size()\n",
        "    plt.plot(daily_counts.index, daily_counts.values)\n",
        "    plt.title('Applications Over Time')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Number of Applications')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# Bar plots for categorical data\n",
        "plt.figure(figsize=(10, 6))\n",
        "df['education'].value_counts().plot(kind='bar')\n",
        "plt.title('Education Distribution')\n",
        "plt.xlabel('Education Level')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Histograms for numerical data\n",
        "plt.figure(figsize=(12, 8))\n",
        "df['age'].hist(bins=30, alpha=0.7, edgecolor='black')\n",
        "plt.title('Age Distribution')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# 3.2 Advanced Visualization with Seaborn\n",
        "print(\"\\nüé® Advanced Visualizations\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Scatter plots with regression\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df, x='age', y='income', hue='education')\n",
        "plt.title('Age vs Income by Education')\n",
        "plt.show()\n",
        "\n",
        "# Box plots for outlier detection\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(data=df, x='education', y='income')\n",
        "plt.title('Income Distribution by Education')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Violin plots for distribution shape\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(data=df, x='employment_status', y='credit_score')\n",
        "plt.title('Credit Score Distribution by Employment Status')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Heatmap for correlation\n",
        "plt.figure(figsize=(10, 8))\n",
        "correlation_matrix = df[numerical_cols].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "# 3.3 Interactive Visualizations with Plotly\n",
        "print(\"\\nüîÑ Interactive Visualizations\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Interactive scatter plot\n",
        "fig = px.scatter(df, x='age', y='income', color='education', \n",
        "                 title='Interactive Age vs Income by Education')\n",
        "fig.show()\n",
        "\n",
        "# Interactive histogram\n",
        "fig = px.histogram(df, x='credit_score', color='default_risk', \n",
        "                   title='Credit Score Distribution by Default Risk')\n",
        "fig.show()\n",
        "\n",
        "# Interactive box plot\n",
        "fig = px.box(df, x='education', y='income', \n",
        "             title='Income Distribution by Education Level')\n",
        "fig.show()\n",
        "\n",
        "print(\"‚úÖ Data visualization completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(f\"üìÖ Current date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Load data from Phase 1 (if available)\n",
        "try:\n",
        "    # Try to load processed data from Phase 1\n",
        "    df = pd.read_parquet('data/processed/dataset_v*.parquet')\n",
        "    print(\"‚úÖ Loaded processed data from Phase 1\")\n",
        "except:\n",
        "    # Create sample data if Phase 1 data not available\n",
        "    print(\"‚ö†Ô∏è  Phase 1 data not found, creating sample data...\")\n",
        "    \n",
        "    # Create sample dataset\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1000\n",
        "    \n",
        "    data = {\n",
        "        'customer_id': range(1, n_samples + 1),\n",
        "        'age': np.random.normal(35, 12, n_samples).astype(int),\n",
        "        'income': np.random.lognormal(10, 0.5, n_samples),\n",
        "        'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples, p=[0.3, 0.4, 0.2, 0.1]),\n",
        "        'employment_status': np.random.choice(['Employed', 'Unemployed', 'Self-employed', 'Retired'], n_samples, p=[0.6, 0.1, 0.2, 0.1]),\n",
        "        'credit_score': np.random.normal(650, 100, n_samples).astype(int),\n",
        "        'loan_amount': np.random.exponential(50000, n_samples),\n",
        "        'loan_duration': np.random.choice([12, 24, 36, 48, 60], n_samples, p=[0.2, 0.3, 0.3, 0.15, 0.05]),\n",
        "        'interest_rate': np.random.normal(8.5, 2, n_samples),\n",
        "        'default_risk': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
        "        'application_date': pd.date_range('2020-01-01', periods=n_samples, freq='D'),\n",
        "        'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_samples, p=[0.2, 0.15, 0.15, 0.15, 0.35]),\n",
        "        'marital_status': np.random.choice(['Single', 'Married', 'Divorced', 'Widowed'], n_samples, p=[0.4, 0.4, 0.15, 0.05]),\n",
        "        'dependents': np.random.poisson(1.5, n_samples),\n",
        "        'previous_loans': np.random.poisson(2, n_samples),\n",
        "        'late_payments': np.random.poisson(0.5, n_samples),\n",
        "        'debt_to_income_ratio': np.random.beta(2, 5, n_samples),\n",
        "        'credit_utilization': np.random.beta(3, 2, n_samples),\n",
        "        'home_ownership': np.random.choice(['Rent', 'Own', 'Mortgage'], n_samples, p=[0.4, 0.2, 0.4]),\n",
        "        'purpose': np.random.choice(['Debt Consolidation', 'Home Improvement', 'Business', 'Education'], n_samples, p=[0.4, 0.2, 0.2, 0.2])\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"‚úÖ Sample dataset created\")\n",
        "\n",
        "print(f\"\\nüìä Dataset Overview:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
